{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMfeiEH4O788"
      },
      "source": [
        "# FedJAX Intro\n",
        "\n",
        "[Open In Colab](https://colab.research.google.com/github/google/fedjax/blob/main/notebooks/fedjax_intro.ipynb)\n",
        "\n",
        "This notebook introduces the basic components of FedJAX and walks through:\n",
        "* What is federated learning?\n",
        "* How to run federated simulations.\n",
        "\n",
        "For more custom use cases (e.g. custom federated algorithm), please refer to [FedJAX Advanced Usages](./fedjax_advanced.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMjinkDX8ZPv"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -q fedjax==0.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZGmIRwGOnU6"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "import fedjax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HyEdeFy1K0X"
      },
      "source": [
        "## What is federated learning?\n",
        "\n",
        "Federated learning (FL) is \"a distributed machine learning approach that trains machine learning models using decentralized examples residing on devices such as smartphones.\"\n",
        "\n",
        "This means that there are two main actors in FL:\n",
        "* Server: Centralized server.\n",
        "* Client: Synonymous with “device”. We use the term “client” from now on.\n",
        "\n",
        "A typical FL algorithm completes the following steps at each training round:\n",
        "1. Server selects a subset of Clients to participate in training.\n",
        "2. Server broadcasts model parameters to the selected Clients.\n",
        "3. Clients complete training on local data.\n",
        "4. Clients send model updates to the Server.\n",
        "5. Server aggregates Client updates into a single update.\n",
        "6. Server uses this aggregation to update the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0FemUmpotUq"
      },
      "source": [
        "## How to run federated simulations\n",
        "\n",
        "In this section, we'll describe how to run federated learning simulations. A federated learning simulation typically consists of the following components:\n",
        "* Federated dataset: a list of client ids and a dataset for each client id.\n",
        "* Model and parameters\n",
        "* Federated algorithm: defines how to train across clients and aggregate multiple client outputs into a single server output\n",
        "\n",
        "We will use the [Federated EMNIST-62](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/emnist/load_data) image recognition task and train our model using the [Federated Averaging](https://arxiv.org/pdf/1602.05629.pdf) algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG1nKEkoqSe8"
      },
      "source": [
        "### Loading the dataset\n",
        "\n",
        "For EMNIST, the examples are grouped by writer source where each writer is treated as a separate client. For more details, see https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/emnist/load_data.\n",
        "\n",
        "FedJAX reuses the already well-defined and supported [`tff.simulation.ClientData`](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/ClientData) construct introduced by TensorFlow Federated (TFF). There are quite a few functionalities baked in to `tff.simulation.ClientData`, but for the most part, we only use `client_ids` and `create_tf_dataset_for_client`.\n",
        "\n",
        "For convenience, we provide `fedjax.datasets.emnist.load_data` as well as other canonical federated datasets under `fedjax.datasets`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD0sYfzhqGZ6",
        "outputId": "6c158b0c-e782-42aa-85a2-12ef1d8ae741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tff-datasets-public/fed_emnist.tar.bz2\n",
            "169811968/169808360 [==============================] - 4s 0us/step\n",
            "client_ids ['f0000_14', 'f0001_41', 'f0005_26', 'f0006_12', 'f0008_45']\n",
            "client_dataset.element_spec OrderedDict([('x', TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None)), ('y', TensorSpec(shape=(), dtype=tf.int32, name=None))])\n"
          ]
        }
      ],
      "source": [
        "federated_train, federated_test = fedjax.datasets.emnist.load_data(\n",
        "    only_digits=False)\n",
        "\n",
        "print('client_ids', federated_train.client_ids[:5])\n",
        "client_id = federated_train.client_ids[0]\n",
        "client_dataset = federated_train.create_tf_dataset_for_client(client_id)\n",
        "print('client_dataset.element_spec', client_dataset.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "PmRSaXZEq3e5",
        "outputId": "bc0fbe9f-34a5-44af-d079-da495cc914bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u003cmatplotlib.image.AxesImage at 0x7f75d293fac8\u003e"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "\u003cFigure size 432x288 with 1 Axes\u003e"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "examples = list(client_dataset.as_numpy_iterator())\n",
        "pixels = examples[-1]['x']\n",
        "label = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'[\n",
        "    examples[-1]['y']]\n",
        "plt.title(f'{label}')\n",
        "plt.imshow(pixels.reshape(28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpH-SYfdyoWG"
      },
      "source": [
        "You'll notice that the output client dataset is a `tf.data.Dataset` of `tf.Tensor`. However, in order to use these examples with JAX, we'll need to convert this to NumPy arrays. Fortunately, TensorFlow provides `tf.data.Dataset.as_numpy_iterator()` which does exactly what we want and converts a `tf.data.Dataset` to an itertor of NumPy arrays.\n",
        "\n",
        "FedJAX also provides a convenient `fedjax.ClientDataHParams` and `fedjax.preprocess_tf_dataset` that takes care of the basic preprocessing usually done on `tf.data.Dataset` (i.e. shuffling, repeating, batching, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggV3aG25ymcd",
        "outputId": "fb19b62c-e4c0-4852-e16a-4e7c8fe25f7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([27, 32, 30, 39,  1, 28, 27, 40, 40, 12], dtype=int32)"
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocessed = fedjax.preprocess_tf_dataset(\n",
        "    client_dataset, fedjax.ClientDataHParams(batch_size=10))\n",
        "batch = next(preprocessed.as_numpy_iterator())\n",
        "batch['y']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF9VXOzEqO--"
      },
      "source": [
        "### Defining the model\n",
        "\n",
        "In this section, we will cover how to intialize models suitable for use in FedJAX. Rather than reimplement core model components (layers, etc.), FedJAX uses a flexible structure that is implementation agnostic and works with multiple popular JAX neural network libraries (such as [Haiku](https://github.com/deepmind/dm-haiku) and [stax](https://jax.readthedocs.io/en/latest/jax.experimental.stax.html)).\n",
        "\n",
        "Similar to datasets, we provide a list of canonical models under `fedjax.models`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcNhQtHTzWCx",
        "outputId": "691ab95b-6182-4910-fc84-f2743d84ce44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/_src/lax/lax.py:6341: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "modules = KeysOnlyKeysView(['linear', 'linear_1', 'linear_2'])\n",
            "backward_pass_output.weight = 10.0\n",
            "metrics[loss] = 4.1264353\n",
            "metrics[weight] = 10.0\n",
            "metrics[accuracy] = 0.0\n"
          ]
        }
      ],
      "source": [
        "model = fedjax.models.emnist.create_dense_model(\n",
        "    only_digits=False, hidden_units=100)\n",
        "\n",
        "# fedjax.PRNGSequence == hk.PRNGSequence\n",
        "# generator of fedjax.PRNGKey\n",
        "rng = next(fedjax.PRNGSequence(0))\n",
        "params = model.init_params(rng)\n",
        "backward_pass_output = model.backward_pass(params, batch, rng)\n",
        "metrics = model.evaluate(params, batch)\n",
        "\n",
        "print('modules =', params.keys())\n",
        "print('backward_pass_output.weight =', backward_pass_output.weight)\n",
        "print('metrics[loss] =', metrics['loss'])\n",
        "print('metrics[weight] =', metrics['weight'])\n",
        "print('metrics[accuracy] =', metrics['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZUAls8_zh8Z"
      },
      "source": [
        "Now, we have a common `fedjax.Model` that can work with later FedJAX functions and utilities. This model has 3 core methods:\n",
        "\n",
        "* `init_params`: Initializes model parameters given PRNGKey.\n",
        "* `backward_pass`: Returns gradients w.r.t. model parameters as well as a few other outputs such as loss and scalar batch weight (typically batch size).\n",
        "* `evaluate`: Runs model forward pass and evaluates output predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVOTv-II18GY"
      },
      "source": [
        "### Federated algorithm\n",
        "\n",
        "Now that we've defined the federated data and the model, we can move on to actually running the federated algorithm. FedJAX provides a `fedjax.FederatedAlgorithm` interface that defines:\n",
        "* `init_state`: Initializes federated algorithm server state\n",
        "* `run_one_round`: Runs one round of federated training\n",
        "\n",
        "Typically, `fedjax.FederatedAlgorithm` will follow very closely to pseudocode found in papers proposing novel algorithms. For this section, we will use the federated averaging implementation already provided by FedJAX at `fedjax.algorithms.FedAvg`.\n",
        "\n",
        "*We introduce `fedjax.Optimizer` here which is just a vanilla container around `optax` optimizer functions that groups them together for convenience.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDpjtJYc2T_D",
        "outputId": "854e84d0-9f0b-4d7e-84d3-c7b93f4d8e6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/_src/lax/lax.py:6341: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 1\n",
            "round 2\n",
            "round 3\n",
            "round 4\n",
            "round 5\n"
          ]
        }
      ],
      "source": [
        "client_optimizer = fedjax.get_optimizer(\n",
        "    fedjax.OptimizerName.SGD, learning_rate=0.1)\n",
        "server_optimizer = fedjax.get_optimizer(\n",
        "    fedjax.OptimizerName.MOMENTUM, learning_rate=1.0, momentum=0.9)\n",
        "hparams = fedjax.algorithms.FedAvgHParams(\n",
        "    fedjax.ClientDataHParams(batch_size=10))\n",
        "rng_seq = fedjax.PRNGSequence(0)\n",
        "federated_averaging = fedjax.algorithms.FedAvg(federated_train, model,\n",
        "                                               client_optimizer,\n",
        "                                               server_optimizer, hparams,\n",
        "                                               rng_seq)\n",
        "\n",
        "# Initialize server state.\n",
        "state = federated_averaging.init_state()\n",
        "\n",
        "# Run multiple federated training rounds.\n",
        "for i in range(1, 6):\n",
        "  # Sample some subset of clients per training round.\n",
        "  client_ids = federated_train.client_ids[:3]\n",
        "  # Run one round of training.\n",
        "  state = federated_averaging.run_round(state, client_ids)\n",
        "  # Do any post processing or evaluation you'd like on output state.\n",
        "  # evaluate(state)\n",
        "  print(f'round {i}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZfi29wI4Gyj"
      },
      "source": [
        "Now this isn't a very useful experiment because it's not doing any intermittent evaluation so there's no real way to see if federated training is progressing properly or not. Of course, you can write your own evaluation logic and other  auxiliary processes (like checkpointing). However, FedJAX provides a lot of these out-of-box in `fedjax.training`. For example, below, is an example of how to run the simulation with `fedjax.training.run_federated_experiment`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsmOUggfGBuW"
      },
      "outputs": [],
      "source": [
        "# Create directory for metrics, checkpoints, and other experiment artifacts.\n",
        "! mkdir /tmp/.federated_experiment\n",
        "\n",
        "# Set logging level to produce colab output.\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IViVpwvF40l2",
        "outputId": "6004ac2d-7f86-4b59-ed50-ddb90b61b041"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/_src/lax/lax.py:6341: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n",
            "INFO:absl:round_num 1: client_ids = ['f1980_17', 'f3930_25', 'f1148_23']\n",
            "INFO:absl:Saving state to /tmp/.federated_experiment/checkpoint_00000001.\n",
            "INFO:absl:round 1 fed_test_eval: loss = 4.0371146\n",
            "INFO:absl:round 1 fed_test_eval: regularizer = 0.0\n",
            "INFO:absl:round 1 fed_test_eval: weight = 86.0\n",
            "INFO:absl:round 1 fed_test_eval: accuracy = 0.093023255\n",
            "INFO:absl:round_num 2: client_ids = ['f3913_01', 'f1346_08', 'f1408_02']\n",
            "INFO:absl:Saving state to /tmp/.federated_experiment/checkpoint_00000002.\n",
            "INFO:absl:round 2 fed_test_eval: loss = 3.74019\n",
            "INFO:absl:round 2 fed_test_eval: regularizer = 0.0\n",
            "INFO:absl:round 2 fed_test_eval: weight = 88.0\n",
            "INFO:absl:round 2 fed_test_eval: accuracy = 0.125\n",
            "INFO:absl:round_num 3: client_ids = ['f1578_22', 'f1135_17', 'f0865_24']\n",
            "INFO:absl:Saving state to /tmp/.federated_experiment/checkpoint_00000003.\n",
            "INFO:absl:round 3 fed_test_eval: loss = 3.5297854\n",
            "INFO:absl:round 3 fed_test_eval: regularizer = 0.0\n",
            "INFO:absl:round 3 fed_test_eval: weight = 47.0\n",
            "INFO:absl:round 3 fed_test_eval: accuracy = 0.23404254\n",
            "INFO:absl:round_num 4: client_ids = ['f0743_31', 'f2410_85', 'f3486_00']\n",
            "INFO:absl:Saving state to /tmp/.federated_experiment/checkpoint_00000004.\n",
            "INFO:absl:round 4 fed_test_eval: loss = 3.3893056\n",
            "INFO:absl:round 4 fed_test_eval: regularizer = 0.0\n",
            "INFO:absl:round 4 fed_test_eval: weight = 69.0\n",
            "INFO:absl:round 4 fed_test_eval: accuracy = 0.10144928\n",
            "INFO:absl:round_num 5: client_ids = ['f2327_90', 'f4098_19', 'f0912_01']\n",
            "INFO:absl:Saving state to /tmp/.federated_experiment/checkpoint_00000005.\n",
            "INFO:absl:round 5 fed_test_eval: loss = 3.0426636\n",
            "INFO:absl:round 5 fed_test_eval: regularizer = 0.0\n",
            "INFO:absl:round 5 fed_test_eval: weight = 51.0\n",
            "INFO:absl:round 5 fed_test_eval: accuracy = 0.4117647\n",
            "INFO:absl:mean_round_duration = 1.650250 sec.\n",
            "INFO:absl:final_eval_duration = 0.000002 sec.\n"
          ]
        }
      ],
      "source": [
        "config = fedjax.training.FederatedExperimentConfig(\n",
        "    root_dir='/tmp/.federated_experiment',\n",
        "    num_rounds=5,\n",
        "    num_clients_per_round=3,\n",
        "    checkpoint_frequency=1,\n",
        "    num_checkpoints_to_keep=5,\n",
        "    eval_frequency=1)\n",
        "periodic_eval_fn_map = collections.OrderedDict(\n",
        "    fed_test_eval=fedjax.training.ClientEvaluationFn(\n",
        "        federated_test.preprocess(lambda ds: ds.batch(128)), model, config))\n",
        "\n",
        "state = fedjax.training.run_federated_experiment(\n",
        "    config, federated_averaging, periodic_eval_fn_map=periodic_eval_fn_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFo8Di9W7TGU"
      },
      "source": [
        "Now, we can examine the `root_dir` to see the artifacts automatically produced by `fedjax.training.run_federated_experiment`.\n",
        "* Checkpoints\n",
        "* TensorFlow event logs for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaBdtr2l55ZS",
        "outputId": "ae3c9037-4df6-47ef-d818-a7caa22b5204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/.federated_experiment:\n",
            "total 3724\n",
            "-rw-r--r-- 1 root root 759887 Dec 29 14:28 checkpoint_00000001\n",
            "-rw-r--r-- 1 root root 759887 Dec 29 14:28 checkpoint_00000002\n",
            "-rw-r--r-- 1 root root 759887 Dec 29 14:28 checkpoint_00000003\n",
            "-rw-r--r-- 1 root root 759887 Dec 29 14:28 checkpoint_00000004\n",
            "-rw-r--r-- 1 root root 759887 Dec 29 14:28 checkpoint_00000005\n",
            "drwxr-xr-x 2 root root   4096 Dec 29 14:28 fed_test_eval\n",
            "\n",
            "/tmp/.federated_experiment/fed_test_eval:\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 1345 Dec 29 14:28 events.out.tfevents.1609252105.f86bbd0dcfcb.60.2498.v2\n"
          ]
        }
      ],
      "source": [
        "! ls -Rl /tmp/.federated_experiment"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "fedjax_intro.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
