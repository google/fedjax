{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMfeiEH4O788"
      },
      "source": [
        "# FedJAX Advanced Usage\n",
        "\n",
        "[Open In Colab](https://colab.research.google.com/github/google/fedjax/blob/main/notebooks/fedjax_advanced.ipynb)\n",
        "\n",
        "This notebook introduces more advanced usages of FedJAX and walks through:\n",
        "* Definining a custom model\n",
        "* Writing a custom federated algorithm\n",
        "\n",
        "This notebook is meant to go deeper into the concepts introduced in [FedJAX Intro](./fedjax_intro.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McswVjK_1_AW"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -q fedjax==0.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZGmIRwGOnU6"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import functools\n",
        "\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "import fedjax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF9VXOzEqO--"
      },
      "source": [
        "## Defining a custom model\n",
        "\n",
        "In this section, we will cover how to use define custom models in a format suitable for use in FedJAX.\n",
        "\n",
        "Below, we use `haiku` as the neural net library of choice. For `haiku`, the following pointers should help:\n",
        "* https://dm-haiku.readthedocs.io/en/latest/api.html#haiku.transform\n",
        "* https://dm-haiku.readthedocs.io/en/latest/api.html#haiku.Module "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr_8xnb9zhpR",
        "outputId": "c2ac23f0-b2e2-4fa9-ffbd-beecc0e3021f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/_src/lax/lax.py:6341: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# parameters = 94862\n",
            "# grads = 94862\n",
            "backward_pass_output.weight = 1.0\n",
            "metrics[loss] = 4.011884\n",
            "metrics[weight] = 1.0\n",
            "metrics[accuracy] = 0.0\n"
          ]
        }
      ],
      "source": [
        "def forward_pass(batch):\n",
        "  \"\"\"Runs forward pass to produce unnormalized logits.\"\"\"\n",
        "  network = hk.Sequential([\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(100),\n",
        "      jax.nn.relu,\n",
        "      hk.Linear(100),\n",
        "      jax.nn.relu,\n",
        "      hk.Linear(62),\n",
        "  ])\n",
        "  return network(batch['x'])\n",
        "\n",
        "\n",
        "def cross_entropy_loss(batch, preds):\n",
        "  targets = batch['y']\n",
        "  num_classes = preds.shape[-1]\n",
        "  log_preds = jax.nn.log_softmax(preds)\n",
        "  one_hot_targets = jax.nn.one_hot(targets, num_classes)\n",
        "  return -jnp.mean(jnp.sum(one_hot_targets * log_preds, axis=-1))\n",
        "\n",
        "\n",
        "def accuracy(batch, preds):\n",
        "  targets = batch['y']\n",
        "  pred_class = jnp.argmax(preds, axis=-1)\n",
        "  return jnp.mean(pred_class == targets)\n",
        "\n",
        "\n",
        "# Transform forward_pass function which uses hk.Module into pure functions.\n",
        "transformed_forward_pass = hk.transform(forward_pass)\n",
        "# Sample batch used to initialize model parameter shapes.\n",
        "sample_batch = collections.OrderedDict(\n",
        "    x=np.ones((1, 28, 28)), y=np.ones((1, 1)))\n",
        "model = fedjax.create_model_from_haiku(\n",
        "    transformed_forward_pass=transformed_forward_pass,\n",
        "    sample_batch=sample_batch,\n",
        "    loss_fn=cross_entropy_loss,\n",
        "    metrics_fn_map=collections.OrderedDict(accuracy=accuracy))\n",
        "\n",
        "rng = next(fedjax.PRNGSequence(0))\n",
        "params = model.init_params(rng)\n",
        "backward_pass_output = model.backward_pass(params, sample_batch, rng)\n",
        "metrics = model.evaluate(params, sample_batch)\n",
        "\n",
        "print('# parameters =', hk.data_structures.tree_size(params))\n",
        "print('# grads =', hk.data_structures.tree_size(backward_pass_output.grads))\n",
        "print('backward_pass_output.weight =', backward_pass_output.weight)\n",
        "print('metrics[loss] =', metrics['loss'])\n",
        "print('metrics[weight] =', metrics['weight'])\n",
        "print('metrics[accuracy] =', metrics['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hRTooGv8CS0"
      },
      "source": [
        "## Writing a custom federated algorithm\n",
        "\n",
        "In this section, we'll go over how to implement your own custom federated algorithm in FedJAX.\n",
        "\n",
        "In order to do this, we will be implementing the Federated Averaging algorithm from scratch.\n",
        "\n",
        "As a refresher, recall that federated algorithms typically consist of:\n",
        "* Client training: How to train across clients on their local data (analogous to the \"map\" in \"mapreduce\").\n",
        "* Server aggregation: How to aggregate multiple client outputs into a single server output (analogous to the \"reduce\" in \"mapreduce\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "equCPaJGYyyT"
      },
      "source": [
        "### Client training\n",
        "\n",
        "In FedJAX, we introduce the `fedjax.ClientTrainer` interface that defines how to conduct training for a **single** client.\n",
        "\n",
        "Below, `SimpleClientTrainer` is an example simple implementation of `fedjax.ClientTrainer`. As you can see, `SimpleClientTrainerState` keeps track of model parameters, optimizer state, and weight at each step, where weight is typically number of examples seen during training. `one_step` simply trains the model parameters on the input batch and updates optimizer state according to the input `client_optimizer`.\n",
        "\n",
        "**NOTE**: `SimpleClientTrainerState` is different from the server state mentioned in federated algorithms. You can think of `SimpleClientTrainerState` as a sort of client state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cn2FSpOYJ_M"
      },
      "outputs": [],
      "source": [
        "SimpleClientTrainerState = collections.namedtuple(\n",
        "    'SimpleClientTrainerState', ['params', 'opt_state', 'weight'])\n",
        "\n",
        "\n",
        "class SimpleClientTrainer(fedjax.ClientTrainer):\n",
        "  \"\"\"Simple client trainer.\"\"\"\n",
        "\n",
        "  def __init__(self, model, client_optimizer):\n",
        "    super().__init__()\n",
        "    self._model = model\n",
        "    self._client_optimizer = client_optimizer\n",
        "\n",
        "  def init_state(self, params, weight=0.):\n",
        "    opt_state = self._client_optimizer.init_fn(params)\n",
        "    return SimpleClientTrainerState(params, opt_state, weight)\n",
        "\n",
        "  # https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#Using-jit-to-speed-up-functions\n",
        "  # https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html \n",
        "  @functools.partial(jax.jit, static_argnums=0)\n",
        "  def one_step(self, client_trainer_state, batch, rng):\n",
        "    backward_pass_output = self._model.backward_pass(\n",
        "        client_trainer_state.params, batch, rng)\n",
        "    params_updates, opt_state = self._client_optimizer.update_fn(\n",
        "        backward_pass_output.grads, client_trainer_state.opt_state)\n",
        "    params = self._client_optimizer.apply_updates(client_trainer_state.params,\n",
        "                                                  params_updates)\n",
        "    weight = client_trainer_state.weight + backward_pass_output.weight\n",
        "    return SimpleClientTrainerState(params, opt_state, weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9toLxtM96tZz"
      },
      "source": [
        "As stated, `fedjax.ClientTrainer` defines how to conduct training for a **single** client. However, we can easily map our `fedjax.ClientTrainer` across multiple clients using `fedjax.train_multiple_clients`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jriKIj3d618c",
        "outputId": "dbd5f528-eec6-4ddb-fbc3-5b0c01a1f804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tff-datasets-public/fed_emnist.tar.bz2\n",
            "169811968/169808360 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/_src/lax/lax.py:6341: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\u003cgenerator object train_multiple_clients at 0x7fbb5316bc50\u003e"
            ]
          },
          "execution_count": 10,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "federated_train, federated_test = fedjax.datasets.emnist.load_data(\n",
        "    only_digits=False)\n",
        "model = fedjax.models.emnist.create_dense_model(\n",
        "    only_digits=False, hidden_units=100)\n",
        "init_params = model.init_params(rng)\n",
        "optimizer = fedjax.get_optimizer(fedjax.OptimizerName.SGD, learning_rate=0.1)\n",
        "\n",
        "client_trainer = SimpleClientTrainer(model, optimizer)\n",
        "init_client_trainer_state = client_trainer.init_state(init_params)\n",
        "\n",
        "# client_outputs contains updated parameters and weight per client.\n",
        "# In this case, client_outputs is SimpleClientTrainerState, but this is\n",
        "# not always guaranteed to be true. It depends on the implementation of\n",
        "# ClientTrainer.\n",
        "client_outputs = fedjax.train_multiple_clients(\n",
        "    federated_train,\n",
        "    federated_train.client_ids[:3],\n",
        "    client_trainer,\n",
        "    init_client_trainer_state,\n",
        "    fedjax.PRNGSequence(0),\n",
        "    fedjax.ClientDataHParams(batch_size=10))\n",
        "\n",
        "client_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe897Hva7hdH"
      },
      "source": [
        "Above, `client_outputs` is a Python generator purposefully. This is to avoid issues of trying to fit `len(client_ids)` copies of the model parameters and optimizer state in memory. For larger models and experiments with larger numbers of clients per federated training rounds, this can be very problematic. Generators give us a nice built-in solution to this.\n",
        "\n",
        "If you are unfamiliar with Python generators, please see https://docs.python.org/3/reference/expressions.html#yieldexpr. To apply any post processing on these client outputs, we recommend using Python's built-in [`map`](https://docs.python.org/3/library/functions.html#map)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61lip-GJ7gwZ",
        "outputId": "d06a442a-2a02-4f81-e680-b1a9f21c8601"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DeviceArray(344., dtype=float32), DeviceArray(372., dtype=float32), DeviceArray(316., dtype=float32)]\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "client_weights = map(lambda co: co.weight, client_outputs)\n",
        "# We call list() to consume the generator.\n",
        "print(list(client_weights))\n",
        "# client_outputs is now an empty generator.\n",
        "print(list(client_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slu870ZK6oqY"
      },
      "source": [
        "### Server aggregation\n",
        "\n",
        "This section describes how to aggregate multiple client outputs into a single output on server. A helpful analogy is to think of client training as the \"map\" in \"mapreduce\" and server aggregation as the \"reduce\".\n",
        "\n",
        "FedJAX provides a few useful utilties for common aggregation strategies. For example, `fedjax.tree_mean` takes an iterator of pytrees and associated weights and returns a weighted average of the pytrees with the same structure.\n",
        "\n",
        "We also use a few JAX utilities for working with [pytrees](https://jax.readthedocs.io/en/latest/pytrees.html) like [`jax.tree_util.tree_multimap`](https://jax.readthedocs.io/en/latest/jax.tree_util.html#jax.tree_util.tree_multimap)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDD7UjxR9Rpd",
        "outputId": "5be5a9bb-f178-4c89-aeb4-5a72d0faa2c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KeysOnlyKeysView(['linear', 'linear_1', 'linear_2'])"
            ]
          },
          "execution_count": 12,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client_outputs = fedjax.train_multiple_clients(\n",
        "    federated_train, federated_train.client_ids[:3], client_trainer,\n",
        "    init_client_trainer_state, fedjax.PRNGSequence(0),\n",
        "    fedjax.ClientDataHParams(batch_size=10))\n",
        "\n",
        "\n",
        "# Weighted average of param delta across clients.\n",
        "def get_delta_params_and_weight(client_output):\n",
        "  delta_params = fedjax.tree_multimap(lambda a, b: a - b, init_params,\n",
        "                                      client_output.params)\n",
        "  return delta_params, client_output.weight\n",
        "\n",
        "\n",
        "delta_params_and_weight = map(get_delta_params_and_weight, client_outputs)\n",
        "delta_params = fedjax.tree_mean(delta_params_and_weight)\n",
        "delta_params.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLG5TaoE_noB"
      },
      "source": [
        "### Putting it all together\n",
        "\n",
        "Now that we've defined how to coordinate training across clients and how to aggregate multiple client outputs, we can put all of the pieces together in a single `fedjax.FederatedAlgorithm`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhoMijWwATI2"
      },
      "outputs": [],
      "source": [
        "FedAvgState = collections.namedtuple('FedAvgState',\n",
        "                                     ['params', 'server_opt_state'])\n",
        "\n",
        "\n",
        "class FedAvg(fedjax.FederatedAlgorithm):\n",
        "  \"\"\"Simple federated averaging algorithm.\"\"\"\n",
        "\n",
        "  def __init__(self, federated_data, model, client_optimizer, server_optimizer,\n",
        "               data_hparams, rng_seq):\n",
        "    self._federated_data = federated_data\n",
        "    self._model = model\n",
        "    self._client_optimizer = client_optimizer\n",
        "    self._server_optimizer = server_optimizer\n",
        "    self._client_data_hparams = client_data_hparams\n",
        "    self._rng_seq = rng_seq\n",
        "    self._client_trainer = SimpleClientTrainer(model, client_optimizer)\n",
        "\n",
        "  @property\n",
        "  def federated_data(self):\n",
        "    return self._federated_data\n",
        "\n",
        "  @property\n",
        "  def model(self):\n",
        "    return self._model\n",
        "\n",
        "  def init_state(self):\n",
        "    params = self._model.init_params(next(self._rng_seq))\n",
        "    server_opt_state = self._server_optimizer.init_fn(params)\n",
        "    return FedAvgState(params, server_opt_state)\n",
        "\n",
        "  def run_round(self, state, client_ids):\n",
        "    \"\"\"Runs one round of federated averaging.\"\"\"\n",
        "    # Train model per client.\n",
        "    client_outputs = fedjax.train_multiple_clients(\n",
        "        self.federated_data, client_ids, self._client_trainer,\n",
        "        self._client_trainer.init_state(state.params), self._rng_seq,\n",
        "        self._client_data_hparams)\n",
        "\n",
        "    # Weighted average of param delta across clients.\n",
        "    def get_delta_params_and_weight(client_output):\n",
        "      delta_params = fedjax.tree_multimap(lambda a, b: a - b, state.params,\n",
        "                                          client_output.params)\n",
        "      return delta_params, client_output.weight\n",
        "\n",
        "    delta_params_and_weight = map(get_delta_params_and_weight, client_outputs)\n",
        "    delta_params = fedjax.tree_mean(delta_params_and_weight)\n",
        "\n",
        "    # Server state update.\n",
        "    updates, server_opt_state = self._server_optimizer.update_fn(\n",
        "        delta_params, state.server_opt_state)\n",
        "    params = self._server_optimizer.apply_updates(state.params, updates)\n",
        "    return FedAvgState(params, server_opt_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYjBa8QoBIHV"
      },
      "source": [
        "To run our simulation, we can use `fedjax.training.run_federated_experiment`. However, for the sake of instruction, here, we'll write the experiment logic from scratch.\n",
        "\n",
        "Running multiple federated training rounds is as simple as calling `run_one_round` inside of a for loop. However, for evaluation, we can make use of some FedJAX utilities, such as:\n",
        "* `fedjax.evaluate_multiple_clients`: Produces generator of evaluation metrics per client\n",
        "* `fedjax.aggregate_metrics`: Aggregates generator of evaluation metrics into a single summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGc32YUiBHpv",
        "outputId": "21096ea4-72b0-44b4-f205-f21745b836c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/_src/lax/lax.py:6341: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "round 0: loss = 3.9508423805236816 accuracy = 0.05982906371355057\n",
            "round 1: loss = 4.171614170074463 accuracy = 0.05982906371355057\n",
            "round 2: loss = 5.593493938446045 accuracy = 0.08547008782625198\n",
            "round 3: loss = 3.534285068511963 accuracy = 0.1367521435022354\n",
            "round 4: loss = 3.030869483947754 accuracy = 0.29914531111717224\n",
            "round 5: loss = 2.7541251182556152 accuracy = 0.3162393271923065\n",
            "round 6: loss = 2.2167229652404785 accuracy = 0.4273504614830017\n",
            "round 7: loss = 1.9401822090148926 accuracy = 0.5213675498962402\n",
            "round 8: loss = 1.8677700757980347 accuracy = 0.5726495981216431\n",
            "round 9: loss = 1.925694465637207 accuracy = 0.5128205418586731\n"
          ]
        }
      ],
      "source": [
        "# Set up fedjax.FederatedAlgorithm.\n",
        "client_optimizer = fedjax.get_optimizer(\n",
        "    fedjax.OptimizerName.SGD, learning_rate=0.1)\n",
        "server_optimizer = fedjax.get_optimizer(\n",
        "    fedjax.OptimizerName.MOMENTUM, learning_rate=1.0, momentum=0.9)\n",
        "client_data_hparams = fedjax.ClientDataHParams(batch_size=10)\n",
        "rng_seq = fedjax.PRNGSequence(0)\n",
        "federated_averaging = FedAvg(federated_train, model, client_optimizer,\n",
        "                             server_optimizer, client_data_hparams, rng_seq)\n",
        "\n",
        "state = federated_averaging.init_state()\n",
        "\n",
        "for i in range(10):\n",
        "  client_ids = federated_train.client_ids[:3]\n",
        "  state = federated_averaging.run_round(state, client_ids)\n",
        "  # Do any post processing or evaluation you'd like on output state.\n",
        "  test_metrics = fedjax.aggregate_metrics(\n",
        "      fedjax.evaluate_multiple_clients(federated_test, client_ids, model,\n",
        "                                       state.params, client_data_hparams))\n",
        "  loss = test_metrics['loss']\n",
        "  accuracy = test_metrics['accuracy']\n",
        "  print(f'round {i}: loss = {loss} accuracy = {accuracy}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "fedjax_advanced.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
